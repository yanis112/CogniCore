#config path
config_path: "config/config.yaml"

# llm parameters
llm_provider: "google" #"github" #"github" #"groq" #"sambanova" #"sambanova" #"groq" # Provider of the LLM model ('cerebras or ollama or groq') don't forget to change the provider if you change the llm. Pay attention the model name can vary depending on the provider !
model_name: "gemini-2.5-flash-preview-04-17" #"meta-Llama-3.1-405B-Instruct" #"llama3-405b" #gpt-4o" #llama-3.1-70b-versatile" #"llama-3.2-90b-text-preview" #"llama3-405b" # "llama-3.2-90b-text-preview" #"llama3-405b" #LLM used for inference the name can be in the following list, pay attention that the llm name should correspond to the llm provider: "hermes3" (ollama);"llama-3.1-8b-instant (groq)"
models_dict: {
            "gemini-2.5-flash-preview-05-20": "google",
            "gemini-2.5-flash-preview-04-17": "google",
            "gemini-2.5-pro-exp-03-25": "google",
            "gemini-2.5-pro-preview-05-06": "google",
              "o3-mini":"github",
              "deepseek-r1-distill-llama-70b":"groq",
              "llama-3.3-70b-versatile":"groq",
              "gpt-4o": "github",
              "meta-Llama-3.1-405B-Instruct": "github",
              "gpt-4o-mini": "github",
              "o1-mini": "github",
              "o1-preview": "github",
              "llama3-405b": "sambanova",
              "DeepSeek-R1-0528": "sambanova",
              "llama-3.1-8b-instant": "groq",
              "llama-3.2-3b-preview": "groq",
              "DeepSeek-R1":"github",
              "llama-4-maverick-17b-128e-instruct": "groq",
              "openai/gpt-4.1": "github",
              "gemma-3n-e4b-it": "google",
              "qwen/qwen3-32b": "groq"
            }

#vllm parameters
vllm_model_name: "gemini-2.5-flash-preview-04-17" #VLLM model to use for la génération
vllm_provider: "gemini" # Provider of the VLLM model
vision_model_dict: { 
    "gemini-2.5-flash-preview-05-20": "gemini",
    "gemini-2.5-flash-preview-04-17": "gemini",
    "gemini-2.5-pro-preview-05-06": "gemini",
    "gemini-2.5-pro-exp-03-25": "gemini",
    "gpt-4o-mini": "github",
    "gpt-4o": "github",
    "phi-4-multimodal-instruct": "github",
    "meta-llama/llama-4-maverick-17b-128e-instruct": "groq",
    "meta-llama/llama-4-scout-17b-16e-instruct": "groq",
    "openai/gpt-4.1": "github",
    "gemini-3n-e4b-it": "gemini"
}

#LLM parameters
cot_enabled: false  # Enable or disable the Chain-Of-Thoughts (COT) feature for the LLM answer
stream: false  # Enable or disable the streaming of the generation (if all the output text is generated and then printed in the interface or if the tokens are generated and printed one by one)
temperature: 1  # Temperature for the generation
top_k: 45 # Top-k for the generation
top_p: 0.95 # Top-p for the generation
max_tokens: 8000 # Maximum number of tokens for the generation (réduit pour respecter la limite du modèle)
llm_token_target: 0 # We give documents to the llm until this number of tokens is reached (sum of the tokens of the documents) , 0 means we don't use this parameter, if autocut is enabled, this parameter is not used !!
save_answer: false # Enable or disable the saving of the answer
prompt_language: "en"  # Language used for all the prompts (and consequently the LLM)


#Community summarisation / entity description parameters
description_model_name: "gemma2:2b" #LLM model to use for generating the entity descriptions (smaller LLMs are faster but less accurate)
description_llm_provider: "ollama" # Provider of the LLM model for the entity descriptions

# Possible actions
actions_dict: {
    "artificial voice creation": "User asks the system to generate a specific artificial voice or audio for a chartacter.",
    "prompt engineering request": "User asks the system to generate a detailed textual prompt for an image creation model.",
    "adapt image description to template": "User asks the system to adapt a given prompt/image_description to a specific prompt template.",
    "make images lifelike": "User requests to transform images into a more realistic representation, or blockbuster movie style.",
    "image2video": "User requests to generate a description of a video animation based on an uploaded image, including action and camera descriptions.",
    "image2sound": "User requests to generate a short prompt or description for sound effects that would match an uploaded image.",
    "remove watermark": "User requests to remove watermarks or logos from something",
    "instagram reel captioning": "User wants to add subtitles/captions to an uploaded video file.",
    "youtube to mp3 conversion": "User provides a YouTube link and requests to download the audio as an MP3 file."
}

#Agentic RAG parameters
query_breaker_model: "o1-mini"
query_breaker_provider: "github"

#Intent classifier parameters
query_classification_model: "meta-llama/llama-4-scout-17b-16e-instruct" #"qwen-2.5-32b"    # "llama-3.3-70b-versatile"
query_classification_provider: "groq"

# =============================
# Parameters for text_classifier_utils.py (TextClassifier)
# =============================
# These parameters are used only by the TextClassifier class in cognicore/text_classifier_utils.py
classification_labels_dict: {
  0: {"class_name": "question", "description": "A general question about any topic."},
  1: {"class_name": "job_search", "description": "A request related to job searching or job offers."},
  2: {"class_name": "internet_search", "description": "A request to search for information on the internet."}
}
classifier_system_prompt: "You are an agent in charge of classifying user's queries into different categories of tasks."

# Watermark removal settings
watermark_removal_output_dir: "C:/Users/Yanis/Documents/AI Art/watermarked_removed"
watermark_margin_percentage: 10
watermark_detection_threshold: 0.2

# Captioned video output settings
captioned_videos_output_dir: "output/captioned_videos"

# =============================
# Parameters for image_classifier_utils.py (ImageClassifier)
# =============================
# These parameters are used only by the ImageClassifier class in cognicore/image_classifier_utils.py
image_classification_labels_dict: {
  0: {"class_name": "animal", "description": "Image contenant un animal."},
  1: {"class_name": "ville", "description": "Image représentant une ville ou un lieu urbain."},
  2: {"class_name": "autre", "description": "Tout autre type d'image."}
}
image_classifier_system_prompt: "You are an agent in charge of classifying images into different categories."
image_classification_model: "gemini-2.5-flash-preview-04-17"
image_classification_provider: "gemini"